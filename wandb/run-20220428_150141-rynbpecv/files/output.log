RoboVisModelV1(
  (w): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=same)
  (w1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=same)
  (a1): Sequential(
    (0): ReLU(inplace=True)
  )
  (w2): Sequential(
    (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=same)
    (1): ReLU(inplace=True)
    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (tr_en_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (linear1): Linear(in_features=32, out_features=32, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=32, out_features=32, bias=True)
    (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (tr_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=32, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=32, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=32, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=32, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (tr_dec_layer): TransformerDecoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (multihead_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (linear1): Linear(in_features=32, out_features=2048, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=2048, out_features=32, bias=True)
    (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (dropout3): Dropout(p=0.1, inplace=False)
  )
  (tr_decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (final_im_func): Sequential(
    (0): Linear(in_features=2048, out_features=32, bias=True)
    (1): ReLU(inplace=True)
  )
  (final_im_map): Sequential(
    (0): Linear(in_features=34, out_features=10, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=10, out_features=1, bias=True)
    (3): Sigmoid()
  )
)
801177
0.052275896072387695
[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
  0%|                                                   | 0/100 [00:00<?, ?it/s]
0.6435053944587708
0.3012319803237915
0.2984144687652588
0.11920002847909927
0.43438470363616943
0.46794700622558594
0.27645403146743774
0.20450004935264587
0.49871140718460083
0.1972515881061554
0.2647876441478729
0.2657025456428528
0.3555794358253479
0.3348458409309387
0.31325313448905945
0.3643428087234497
0.27887484431266785
0.44467365741729736
0.2505212724208832
0.4040413796901703
0.2652536630630493
0.3859686851501465
0.24562057852745056
0.3442160189151764
0.24193957448005676
0.43051353096961975
0.22894276678562164
0.32160791754722595
0.3549879789352417
0.3666080832481384
0.26725491881370544
0.32825130224227905
0.16525088250637054
0.345510870218277
0.290010541677475
0.29493820667266846
0.35538193583488464
0.24903392791748047
0.23204335570335388
0.324374794960022
0.23723305761814117
0.2558026909828186
0.2491043657064438
0.2031865417957306
0.25866982340812683
0.20571109652519226
0.4807196855545044
0.2785290479660034

  0%|                                                   | 0/100 [03:07<?, ?it/s]
0.3646162748336792
  0%|                                                   | 0/100 [00:00<?, ?it/s]
0.283896803855896
0.3226635754108429
0.40272849798202515
0.1995861977338791
0.3120155930519104
0.2950340509414673
0.26775074005126953
0.2901175618171692
0.23115573823451996
0.32913488149642944
0.22607427835464478
0.23330116271972656

  0%|                                                   | 0/100 [00:54<?, ?it/s]
  0%|                                                   | 0/100 [00:00<?, ?it/s]
0.3272313177585602
0.4335542321205139
0.434137225151062
0.3962029218673706
0.3805984854698181
0.22870969772338867
0.3819807767868042
0.33047330379486084
0.3612176179885864
0.17675209045410156
0.24744680523872375
0.19168823957443237
0.2928358018398285
0.3738579750061035
0.40914851427078247
0.23110367357730865
0.2547508776187897
0.3276972770690918
0.32763534784317017
0.20179308950901031
0.25605911016464233
0.1436554193496704
0.30058956146240234
0.23218852281570435
0.306915819644928
0.18576408922672272
0.2915763258934021
0.24895580112934113
0.3833196461200714
0.20024792850017548
0.2700370252132416
0.45431843400001526
0.2649054229259491
0.2051885724067688
0.34520387649536133
0.29786115884780884
0.222863107919693
0.29089462757110596
0.26046884059906006
0.26898717880249023
0.3330497145652771
0.3402157425880432
0.18573187291622162
0.3004297614097595
0.2864381968975067
0.23160649836063385
0.24043607711791992
0.3088713586330414

  1%|▍                                       | 1/100 [03:29<5:46:17, 209.87s/it]
0.2811179757118225
loss: 8.579039786127395e-08
0.375458687543869
0.35454583168029785
0.317365825176239
0.33983492851257324
0.24911218881607056
0.3912925124168396
0.26477688550949097
0.1936621218919754
0.20801714062690735
0.23484735190868378
0.18423455953598022
0.3031136393547058
0.22839492559432983
0.330502986907959
0.24514415860176086
0.24200238287448883
0.316769003868103
0.3308752179145813
0.33392223715782166
0.35515040159225464
0.344738632440567
0.16867028176784515
0.27280551195144653
0.29921120405197144
0.3858540654182434
0.26550745964050293
0.31353944540023804
0.32767897844314575
0.23675748705863953
0.32360291481018066
0.2649933993816376
0.23867321014404297
0.23519855737686157
0.28894996643066406
0.28673022985458374
0.2982015013694763
0.20265598595142365
0.22283847630023956
0.22906804084777832
0.272220253944397
0.29031333327293396
0.2685743570327759
0.2677087187767029
0.2606043219566345
0.24575364589691162
0.26793503761291504
0.30974650382995605
0.2238263189792633

  2%|▊                                       | 2/100 [06:54<5:37:49, 206.84s/it]
0.27278512716293335
loss: 8.324741429532878e-08
0.23450759053230286
0.24759089946746826
0.2628478705883026
0.23397184908390045
0.198434978723526
0.25158485770225525
0.22522631287574768
0.19135098159313202
0.25063881278038025
0.2151813507080078
0.3309091031551361
0.2730579972267151
0.22216200828552246
0.17365556955337524
0.23715755343437195
0.22628913819789886
0.2841792702674866
0.23875612020492554
0.34946197271347046
0.16451503336429596
0.2346218228340149
0.19618907570838928
0.23877456784248352
0.36543479561805725
0.3660883605480194

  2%|▊                                       | 2/100 [08:46<7:10:10, 263.37s/it]
0.16126549243927002
  0%|                                                   | 0/100 [00:00<?, ?it/s]
  0%|                                                   | 0/100 [00:00<?, ?it/s]Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 125, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 405, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
0.1779409795999527
0.282387912273407
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 114, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 395, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/Users/gmmyung/miniconda3/envs/gazetrack/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
0.24727752804756165
0.21221531927585602
0.1988726556301117
0.17543746531009674
0.18449097871780396
0.22069494426250458
0.19241753220558167
0.30831894278526306
0.3099949359893799
0.2496456652879715
0.235557422041893
0.27965009212493896
0.14016321301460266
0.2843412756919861
0.32567477226257324
0.2516726851463318
0.24152407050132751
0.3292694091796875
0.33026713132858276
0.3611868619918823
0.28806400299072266
0.30609384179115295
0.2802070379257202
0.26553311944007874
0.2915029525756836
0.2519248425960541
0.28011155128479004
0.27251508831977844
0.2438570261001587
0.2394813597202301
0.31320875883102417
0.2871425449848175
0.2813369035720825
0.2293873131275177
0.284801185131073
0.25404712557792664
0.2782967686653137
0.28425219655036926
0.280487060546875
0.27759671211242676
0.2693142294883728
0.257114052772522
0.25412052869796753
0.28472161293029785
0.26788705587387085
0.2892768383026123
  0%|                                                   | 0/100 [03:28<?, ?it/s]
0.22879889607429504
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
  0%|                                                   | 0/100 [00:00<?, ?it/s]
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
0.22294561564922333
0.2832273244857788

  0%|                                                   | 0/100 [00:13<?, ?it/s]
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
  0%|                                                   | 0/100 [00:00<?, ?it/s]
0.22075003385543823
0.2617890238761902
0.25019487738609314
0.24071770906448364
0.25238776206970215
0.27041858434677124
0.2775539755821228
0.26819390058517456
0.26322510838508606
0.23727750778198242
0.2009439766407013
0.27389079332351685
0.24761584401130676
0.26242148876190186
0.29824578762054443
0.2825466990470886
0.2900054156780243
0.3167519271373749
0.241563081741333
0.28361666202545166
0.20325496792793274
0.24726608395576477
0.3087477684020996
0.2655576467514038
0.26861244440078735
0.25230032205581665
0.2312307357788086
0.2913385331630707

  0%|                                                   | 0/100 [02:06<?, ?it/s]
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fe5b9bb5e50> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fe5bb57c4c0> (for post_run_cell):